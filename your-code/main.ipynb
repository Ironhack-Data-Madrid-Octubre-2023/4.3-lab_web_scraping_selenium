{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Selenium Doc](https://www.selenium.dev/documentation/)\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `Selenium` and `pandas` are imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable the options you may need. In the next cell you have an example of them but you can choose to use them or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver configuration\n",
    "opciones=Options()\n",
    "\n",
    "opciones.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "opciones.add_experimental_option('useAutomationExtension', False)\n",
    "opciones.headless=False    # si True, no aperece la ventana (headless=no visible)\n",
    "opciones.add_argument('--start-maximized')         # comienza maximizado\n",
    "#opciones.add_argument('user-data-dir=selenium')    # mantiene las cookies\n",
    "#opciones.add_extension('driver_folder/adblock.crx')       # adblocker\n",
    "opciones.add_argument('--incognito')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WebDriver.__init__() got multiple values for argument 'options'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[178], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./chromedriver.exe\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#remember substitute this for your driver path\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(driver,options \u001b[38;5;241m=\u001b[39m opciones)\n",
      "\u001b[1;31mTypeError\u001b[0m: WebDriver.__init__() got multiple values for argument 'options'"
     ]
    }
   ],
   "source": [
    "driver = \"./chromedriver.exe\" #remember substitute this for your driver path\n",
    "driver = webdriver.Chrome(driver,options = opciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse, and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.chrome.webdriver.WebDriver (session=\"b4c6c1c4d2774a3445b0e288c2f17243\")>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "PATH = 'driver/ChromeDriver.exe'\n",
    "\n",
    "driver = webdriver.Chrome()  \n",
    "\n",
    "driver.get('https://github.com/trending/developers')\n",
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ismail Pelaseyed', 'homanp']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.find_elements(By.CSS_SELECTOR, 'div.col-md-6')[0].text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = [e.text.split('\\n') for e in driver.find_elements(By.CSS_SELECTOR, 'div.col-md-6')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ismail Pelaseyed', 'homanp'],\n",
       " ['Alex Stokes', 'ralexstokes'],\n",
       " ['Don Jayamanne', 'DonJayamanne'],\n",
       " ['Stefan Prodan', 'stefanprodan'],\n",
       " ['Travis Cline', 'tmc'],\n",
       " ['Oliver', 'SchrodingersGat'],\n",
       " ['dgtlmoon'],\n",
       " ['Guillaume Klein', 'guillaumekln'],\n",
       " ['lllyasviel'],\n",
       " ['Angelos Chalaris', 'Chalarangelo'],\n",
       " ['Jon Skeet', 'jskeet'],\n",
       " ['Chris Banes', 'chrisbanes'],\n",
       " ['Carlos Scheidegger', 'cscheid'],\n",
       " ['Anders Eknert', 'anderseknert'],\n",
       " ['Emil Ernerfeldt', 'emilk'],\n",
       " ['Andrea Aime', 'aaime'],\n",
       " ['Works for GeoSolutions', 'GeoSolutions'],\n",
       " ['Mattt', 'mattt'],\n",
       " ['Meng Zhang', 'wsxiaoys'],\n",
       " ['Shahed Nasser', 'shahednasser'],\n",
       " ['dkhamsing'],\n",
       " ['Kieron Quinn', 'KieronQuinn'],\n",
       " ['afc163', 'afc163'],\n",
       " ['Alan Donovan', 'adonovan'],\n",
       " ['Lee Calcote', 'leecalcote'],\n",
       " ['Works for Layer5', 'Layer5'],\n",
       " ['Costa Huang', 'vwxyzjn']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e for e in lista if len(e)<3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "driver = webdriver.Chrome()  \n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.CSS_SELECTOR, 'body > div.logged-out.env-production.page-responsive > div.application-main > main > div.position-relative.container-lg.p-responsive.pt-6 > div > div.Box-header.d-lg-flex.flex-items-center.flex-justify-between > nav > a:nth-child(1)').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_elements(By.XPATH, '//h2//a')[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'juspay / hyperswitch'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.find_element(By.XPATH, '/html/body/div[1]/div[4]/main/div[3]/div/div[2]/article[1]/h2/a').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['juspay / hyperswitch',\n",
       " 'antirez / smallchat',\n",
       " 'swisskyrepo / PayloadsAllTheThings',\n",
       " 'celestiaorg / celestia-node',\n",
       " 'THUDM / ChatGLM3',\n",
       " 'PaddlePaddle / PaddleOCR',\n",
       " 'practical-tutorials / project-based-learning',\n",
       " 'php-youtubers / directory',\n",
       " 'cisagov / LME',\n",
       " 'InterviewReady / system-design-resources',\n",
       " 'langchain-ai / langchain',\n",
       " 'langchain-ai / langserve',\n",
       " 'dataelement / bisheng',\n",
       " 'projectdiscovery / nuclei-templates',\n",
       " 'Las-Fuerzas-Del-Cielo / Sistema-Anti-Fraude-Electoral',\n",
       " 'highcharts / highcharts',\n",
       " 'microsoft / PowerToys',\n",
       " '1c7 / chinese-independent-developer',\n",
       " 'Azure / MS-AMP',\n",
       " 'microsoft / Web-Dev-For-Beginners',\n",
       " 'xxlong0 / Wonder3D',\n",
       " 'Azure / Azure-Sentinel',\n",
       " 'yunjey / pytorch-tutorial',\n",
       " 'atherosai / ui',\n",
       " 'facebookresearch / llama']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.text for e in driver.find_elements(By.XPATH, '//h2//a')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "driver.get(url)\n",
    "tabla=driver.find_elements(By.XPATH, '//a//img')\n",
    "#tabla.get_attribute('href') no funciona hay que hacer loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/static/images/icons/wikipedia.png\n",
      "https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-wordmark-en.svg\n",
      "https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-tagline-en.svg\n",
      "https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png\n",
      "https://upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg/220px-Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg\n",
      "https://upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/220px-Steamboat-willie.jpg\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Disney_Oscar_1953_%28cropped%29.jpg/170px-Disney_Oscar_1953_%28cropped%29.jpg\n",
      "https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png\n",
      "https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Disneyland_Resort_logo.svg/135px-Disneyland_Resort_logo.svg.png\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/20px-Animation_disc.svg.png\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/15px-Magic_Kingdom_castle.jpg\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/21px-Blank_television_set.svg.png\n",
      "https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png\n",
      "https://en.wikipedia.org/static/images/footer/wikimedia-button.png\n",
      "https://en.wikipedia.org/static/images/footer/poweredby_mediawiki_88x31.png\n"
     ]
    }
   ],
   "source": [
    "for elemento in tabla:\n",
    "    src = elemento.get_attribute('src')\n",
    "    print(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"bfac182c87a5a9ce70a607280b699fd3\", element=\"F2A0CF58D00A2EAD58B3A3E3DE1E3365_element_28055\")>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "driver.get(url)\n",
    "tabla=driver.find_element(By.ID, 'bodyContent')\n",
    "\n",
    "tabla#.find_elements(By.TAG_NAME, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wiktionary.org/wiki/Python\n",
      "https://en.wiktionary.org/wiki/python\n",
      "https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=1&editintro=Template:Disambig_editintro\n",
      "https://en.wikipedia.org/wiki/Pythonidae\n",
      "https://en.wikipedia.org/wiki/Python_(genus)\n",
      "https://en.wikipedia.org/wiki/Python_(mythology)\n",
      "https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=2&editintro=Template:Disambig_editintro\n",
      "https://en.wikipedia.org/wiki/Python_(programming_language)\n",
      "https://en.wikipedia.org/wiki/CMU_Common_Lisp\n",
      "https://en.wikipedia.org/wiki/PERQ#PERQ_3\n",
      "https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=3&editintro=Template:Disambig_editintro\n",
      "https://en.wikipedia.org/wiki/Python_of_Aenus\n",
      "https://en.wikipedia.org/wiki/Python_(painter)\n",
      "https://en.wikipedia.org/wiki/Python_of_Byzantium\n",
      "https://en.wikipedia.org/wiki/Python_of_Catana\n",
      "https://en.wikipedia.org/wiki/Python_Anghelo\n",
      "https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=4&editintro=Template:Disambig_editintro\n",
      "https://en.wikipedia.org/wiki/Python_(Efteling)\n",
      "https://en.wikipedia.org/wiki/Python_(Busch_Gardens_Tampa_Bay)\n",
      "https://en.wikipedia.org/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)\n",
      "https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=5&editintro=Template:Disambig_editintro\n",
      "https://en.wikipedia.org/wiki/Python_(automobile_maker)\n",
      "https://en.wikipedia.org/wiki/Python_(Ford_prototype)\n",
      "https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=6&editintro=Template:Disambig_editintro\n",
      "https://en.wikipedia.org/wiki/Python_(missile)\n",
      "https://en.wikipedia.org/wiki/Python_(nuclear_primary)\n",
      "https://en.wikipedia.org/wiki/Colt_Python\n",
      "https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=7&editintro=Template:Disambig_editintro\n",
      "https://en.wikipedia.org/wiki/Python_(codename)\n",
      "https://en.wikipedia.org/wiki/Python_(film)\n",
      "https://en.wikipedia.org/wiki/Monty_Python\n",
      "https://en.wikipedia.org/wiki/Python_(Monty)_Pictures\n",
      "https://en.wikipedia.org/wiki/Timon_of_Phlius\n",
      "https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=8&editintro=Template:Disambig_editintro\n",
      "https://en.wikipedia.org/wiki/Pyton\n",
      "https://en.wikipedia.org/wiki/Pithon\n",
      "https://en.wikipedia.org/wiki/File:Disambig_gray.svg\n",
      "https://en.wikipedia.org/wiki/Help:Disambiguation\n",
      "https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0\n",
      "https://en.wikipedia.org/w/index.php?title=Python&oldid=1182751793\n",
      "https://en.wikipedia.org/wiki/Help:Category\n",
      "https://en.wikipedia.org/wiki/Category:Disambiguation_pages\n",
      "https://en.wikipedia.org/wiki/Category:Human_name_disambiguation_pages\n",
      "https://en.wikipedia.org/wiki/Category:Disambiguation_pages_with_given-name-holder_lists\n",
      "https://en.wikipedia.org/wiki/Category:Short_description_is_different_from_Wikidata\n",
      "https://en.wikipedia.org/wiki/Category:All_article_disambiguation_pages\n",
      "https://en.wikipedia.org/wiki/Category:All_disambiguation_pages\n",
      "https://en.wikipedia.org/wiki/Category:Animal_common_name_disambiguation_pages\n"
     ]
    }
   ],
   "source": [
    "for elemento in tabla.find_elements(By.TAG_NAME, 'a'):\n",
    "    href = elemento.get_attribute('href')\n",
    "    print(href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title 20 - Education', \"Title 38 - Veterans' Benefits ٭\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "driver.get(url)\n",
    "tabla=driver.find_elements(By.CLASS_NAME, 'usctitlechanged')\n",
    "print([e.text for e in tabla])\n",
    "len([e.text for e in tabla])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARNOLDO JIMENEZ',\n",
       " 'OMAR ALEXANDER CARDENAS',\n",
       " 'YULAN ADONAY ARCHAGA CARIAS',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'DONALD EUGENE FIELDS II',\n",
       " 'RUJA IGNATOVA',\n",
       " 'WILVER VILLEGAS-PALOMINO',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code \n",
    "[e.text for e in driver.find_elements(By.CSS_SELECTOR, 'h3.title')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"bfac182c87a5a9ce70a607280b699fd3\", element=\"FFF2F91A37165E0AA3EED9F43021B994_element_3713\")>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "tabla = driver.find_element(By.XPATH, '/html/body/div[3]/div[6]')\n",
    "tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Citizen\\nresponse\\nDate & Time\\nUTC\\nLat.\\ndegrees\\nLon.\\ndegrees\\nDepth\\nkm\\nMag.[+] Region'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnas = tabla.find_element(By.TAG_NAME, 'thead').text\n",
    "columnas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-11-01 15:03:32\\n11 min ago\\n40.528 -123.568 5 2.4 NORTHERN CALIFORNIA\\n2023-11-01 14:49:46\\n25 min ago\\n59.976 10.946 8 3.0 SOUTHERN NORWAY\\n2023-11-01 14:45:25\\n29 min ago\\n0.140 124.260 75 3.5 MINAHASA, SULAWESI, INDONESIA\\n2023-11-01 14:38:02\\n37 min ago\\n36.839 -3.007 13 1.8 STRAIT OF GIBRALTAR\\n2023-11-01 14:35:25\\n39 min ago\\n13.820 120.520 50 3.6 MINDORO, PHILIPPINES\\n2023-11-01 14:33:41\\n41 min ago\\n37.610 21.810 5 2.5 SOUTHERN GREECE\\n2023-11-01 14:30:21\\n44 min ago\\n19.402 -155.264 4 2.0 ISLAND OF HAWAII, HAWAII\\n2023-11-01 14:29:27\\n45 min ago\\n37.875 36.335 3 2.1 CENTRAL TURKEY\\n2023-11-01 14:26:30\\n48 min ago\\n-24.003 -66.809 218 3.0 JUJUY, ARGENTINA\\n2023-11-01 14:10:58\\n1 hr 04 min ago\\n19.483 -155.489 -0 2.5 ISLAND OF HAWAII, HAWAII\\n2023-11-01 14:09:01\\n1 hr 06 min ago\\n36.796 -3.026 9 2.2 STRAIT OF GIBRALTAR\\n2023-11-01 14:07:48\\n1 hr 07 min ago\\n-8.333 121.545 13 5.1 FLORES REGION, INDONESIA\\n2023-11-01 14:00:40\\n1 hr 14 min ago\\n45.943 13.823 6 1.2 SLOVENIA\\n2023-11-01 13:57:28\\n1 hr 17 min ago\\n37.870 36.359 5 2.7 CENTRAL TURKEY\\n2023-11-01 13:50:15\\n1 hr 24 min ago\\n45.937 10.947 6 1.1 NORTHERN ITALY\\n2023-11-01 13:46:29\\n1 hr 28 min ago\\n39.161 20.542 4 2.0 GREECE\\n2023-11-01 13:44:37\\n1 hr 30 min ago\\n-2.960 128.840 10 2.6 CERAM SEA, INDONESIA\\n2023-11-01 13:43:45\\n1 hr 31 min ago\\n35.760 26.920 33 2.2 CRETE, GREECE\\n2023-11-01 13:27:52\\n1 hr 47 min ago\\n41.439 28.428 3 2.4 WESTERN TURKEY\\n2023-11-01 13:25:48\\n1 hr 49 min ago\\n-2.820 135.470 10 3.8 NEAR N COAST OF PAPUA, INDONESIA\\n2023-11-01 13:22:17\\n1 hr 52 min ago\\n-20.340 -69.000 102 2.9 TARAPACA, CHILE\\n2023-11-01 13:11:43\\n2 hr 03 min ago\\n48.640 155.950 10 4.0 KURIL ISLANDS\\n2023-11-01 12:58:48\\n2 hr 16 min ago\\n36.036 36.103 5 2.9 TURKEY-SYRIA BORDER REGION\\n2023-11-01 12:53:08\\n2 hr 22 min ago\\n38.080 23.480 7 2.0 GREECE\\n2023-11-01 12:51:15\\n2 hr 23 min ago\\n10.925 93.060 94 5.1 ANDAMAN ISLANDS, INDIA REGION\\n2023-11-01 12:49:31\\n2 hr 25 min ago\\n-12.400 -77.020 54 3.7 NEAR COAST OF CENTRAL PERU\\n2023-11-01 12:48:50\\n2 hr 26 min ago\\n-5.090 104.130 10 2.5 SOUTHERN SUMATRA, INDONESIA\\n2023-11-01 12:31:58\\n2 hr 43 min ago\\n38.890 23.310 10 2.0 GREECE\\n2023-11-01 12:25:13\\n2 hr 49 min ago\\n50.306 12.367 10 2.4 GERMANY\\n2023-11-01 12:22:38\\n2 hr 52 min ago\\n36.765 -3.002 11 2.2 STRAIT OF GIBRALTAR\\n2023-11-01 12:01:45\\n3 hr 13 min ago\\n37.470 22.230 194 2.6 SOUTHERN GREECE\\n2023-11-01 12:00:41\\n3 hr 14 min ago\\n11.850 -86.440 79 2.7 NEAR COAST OF NICARAGUA\\n2023-11-01 11:54:44\\n3 hr 20 min ago\\n37.466 37.274 2 2.4 CENTRAL TURKEY\\n2023-11-01 11:52:50\\n3 hr 22 min ago\\n38.775 40.173 13 2.4 EASTERN TURKEY\\n2023-11-01 11:40:41\\n3 hr 34 min ago\\n-8.970 109.890 10 3.3 JAVA, INDONESIA\\n2023-11-01 11:35:37\\n3 hr 39 min ago\\n61.345 -146.956 32 2.5 SOUTHERN ALASKA\\n2023-11-01 11:35:22\\n3 hr 39 min ago\\n-8.920 109.990 10 3.2 JAVA, INDONESIA\\n2023-11-01 11:27:11\\n3 hr 48 min ago\\n-8.030 107.370 23 3.2 JAVA, INDONESIA\\n2023-11-01 11:24:04\\n3 hr 51 min ago\\n37.059 -1.885 0 1.5 SPAIN\\n2023-11-01 11:15:01\\n4 hr 00 min ago\\n36.659 -116.267 8 2.7 NEVADA\\n2023-11-01 11:10:53\\n4 hr 04 min ago\\n38.816 -122.818 2 2.0 NORTHERN CALIFORNIA\\n2023-11-01 10:54:24\\n4 hr 20 min ago\\n18.650 -103.260 11 3.7 MICHOACAN, MEXICO\\n2023-11-01 10:47:26\\n4 hr 27 min ago\\n47.353 6.911 4 1.9 FRANCE\\n2023-11-01 10:46:13\\n4 hr 28 min ago\\n35.858 -98.049 5 2.3 OKLAHOMA\\n2023-11-01 10:40:15\\n4 hr 34 min ago\\n1.549 129.288 10 4.9 HALMAHERA, INDONESIA\\n2023-11-01 10:39:47\\n4 hr 35 min ago\\n32.110 59.730 10 4.7 EASTERN IRAN\\n2023-11-01 10:29:47\\n4 hr 45 min ago\\n19.650 -104.280 45 3.6 JALISCO, MEXICO\\n2023-11-01 10:24:52\\n4 hr 50 min ago\\n-35.756 179.285 239 3.3 OFF E. COAST OF N. ISLAND, N.Z.\\n2023-11-01 10:18:14\\n4 hr 56 min ago\\n46.305 13.600 6 1.3 SLOVENIA\\n2023-11-01 10:15:43\\n4 hr 59 min ago\\n-15.517 -177.518 410 5.1 FIJI REGION\\n2023-11-01 10:12:56\\n5 hr 02 min ago\\n17.810 -103.020 16 3.8 OFFSHORE MICHOACAN, MEXICO\\n2023-11-01 10:06:48\\n5 hr 08 min ago\\n35.364 33.441 24 1.5 CYPRUS REGION\\n2023-11-01 09:57:14\\n5 hr 17 min ago\\n17.932 -66.287 13 2.7 PUERTO RICO REGION\\n2023-11-01 09:40:30\\n5 hr 34 min ago\\n-31.770 -69.090 90 3.1 SAN JUAN, ARGENTINA\\n2023-11-01 09:40:23\\n5 hr 34 min ago\\n-2.110 120.920 10 3.3 SULAWESI, INDONESIA\\n2023-11-01 09:36:41\\n5 hr 38 min ago\\n16.950 -99.640 44 3.7 GUERRERO, MEXICO\\n2023-11-01 09:35:57\\n5 hr 39 min ago\\n34.830 24.820 20 2.8 CRETE, GREECE\\n2023-11-01 09:35:48\\n5 hr 39 min ago\\n30.500 -116.180 19 3.6 OFFSHORE BAJA CALIFORNIA, MEXICO\\n2023-11-01 09:20:50\\n5 hr 54 min ago\\n39.406 28.446 5 2.1 WESTERN TURKEY\\n2023-11-01 09:15:41\\n5 hr 59 min ago\\n25.620 -100.720 5 3.9 COAHUILA, MEXICO\\n2023-11-01 09:10:44\\n6 hr 04 min ago\\n27.667 -18.256 41 2.1 CANARY ISLANDS, SPAIN REGION\\n2023-11-01 08:55:19\\n6 hr 19 min ago\\n37.816 36.459 7 2.7 CENTRAL TURKEY\\n2023-11-01 08:44:01\\n6 hr 31 min ago\\n39.086 33.067 5 2.2 CENTRAL TURKEY\\n2023-11-01 08:42:42\\n6 hr 32 min ago\\n-41.665 174.231 12 3.1 COOK STRAIT, NEW ZEALAND\\n2023-11-01 08:41:30\\n6 hr 33 min ago\\n38.700 142.000 60 3.7 NEAR EAST COAST OF HONSHU, JAPAN\\n2023-11-01 08:38:36\\n6 hr 36 min ago\\n-31.632 178.218 33 4.4 KERMADEC ISLANDS REGION\\n2023-11-01 08:22:47\\n6 hr 52 min ago\\n38.900 22.610 77 2.0 GREECE\\n2023-11-01 08:18:25\\n6 hr 56 min ago\\n-28.770 -71.500 44 3.5 OFFSHORE ATACAMA, CHILE\\n2023-11-01 08:16:57\\n6 hr 58 min ago\\n19.241 -155.398 31 2.1 ISLAND OF HAWAII, HAWAII\\n2023-11-01 08:10:09\\n7 hr 05 min ago\\n14.640 -93.820 24 3.9 OFF COAST OF CHIAPAS, MEXICO\\n2023-11-01 08:08:47\\n7 hr 06 min ago\\n44.361 148.997 44 4.9 KURIL ISLANDS\\n2023-11-01 08:06:29\\n7 hr 08 min ago\\n15.980 -94.080 86 3.7 OFFSHORE CHIAPAS, MEXICO\\n2023-11-01 08:05:18\\n7 hr 09 min ago\\n42.855 13.224 5 2.1 CENTRAL ITALY\\n2023-11-01 07:57:07\\n7 hr 18 min ago\\n36.104 28.808 8 2.6 DODECANESE IS.-TURKEY BORDER REG\\n2023-11-01 07:57:07\\n7 hr 18 min ago\\n39.680 24.350 10 3.0 AEGEAN SEA\\n2023-11-01 07:56:39\\n7 hr 18 min ago\\n18.390 -102.700 58 3.7 MICHOACAN, MEXICO\\n2023-11-01 07:53:43\\n7 hr 21 min ago\\n36.113 35.785 7 3.1 CENTRAL TURKEY\\n2023-11-01 07:44:34\\n7 hr 30 min ago\\n38.200 22.690 5 2.1 GREECE\\n2023-11-01 07:43:45\\n7 hr 31 min ago\\n61.543 -147.799 10 3.1 SOUTHERN ALASKA\\n2023-11-01 07:38:27\\n7 hr 36 min ago\\n53.760 159.460 140 3.5 NEAR EAST COAST OF KAMCHATKA\\n2023-11-01 07:31:35\\n7 hr 43 min ago\\n0.820 122.550 20 2.5 MINAHASA, SULAWESI, INDONESIA\\n2023-11-01 07:29:26\\n7 hr 45 min ago\\n17.050 -94.180 140 3.5 OAXACA, MEXICO\\n2023-11-01 07:19:30\\n7 hr 55 min ago\\n10.931 -86.443 21 3.6 OFF COAST OF COSTA RICA\\n2023-11-01 07:11:43\\n8 hr 03 min ago\\n-32.850 -178.910 44 4.8 SOUTH OF KERMADEC ISLANDS\\n2023-11-01 07:10:03\\n8 hr 05 min ago\\n35.619 1.363 0 3.0 NORTHERN ALGERIA\\n2023-11-01 07:04:41\\n8 hr 10 min ago\\n38.892 15.674 180 2.4 SICILY, ITALY\\n2023-11-01 07:02:42\\n8 hr 12 min ago\\n38.840 23.510 5 2.1 GREECE\\n2023-11-01 06:53:03\\n8 hr 22 min ago\\n38.047 36.861 5 2.2 CENTRAL TURKEY\\n2023-11-01 06:52:57\\n8 hr 22 min ago\\n33.040 76.070 5 3.2 KASHMIR-INDIA BORDER REGION\\n2023-11-01 06:51:44\\n8 hr 23 min ago\\n-10.917 166.569 70 5.2 SANTA CRUZ ISLANDS\\n2023-11-01 06:43:30\\n8 hr 31 min ago\\n8.995 -84.027 26 3.0 OFF COAST OF COSTA RICA\\n2023-11-01 06:30:33\\n8 hr 44 min ago\\n12.620 -88.180 28 2.7 OFFSHORE EL SALVADOR\\n2023-11-01 06:29:38\\n8 hr 45 min ago\\n-10.950 166.629 74 4.6 SANTA CRUZ ISLANDS\\n2023-11-01 06:23:24\\n8 hr 51 min ago\\n18.110 -103.310 14 3.6 OFFSHORE MICHOACAN, MEXICO\\n2023-11-01 06:22:42\\n8 hr 52 min ago\\n15.280 -93.760 11 3.8 OFFSHORE CHIAPAS, MEXICO\\n2023-11-01 06:03:35\\n9 hr 11 min ago\\n46.389 12.731 8 1.0 NORTHERN ITALY\\n2023-11-01 06:03:26\\n9 hr 11 min ago\\n17.400 -94.710 136 3.8 VERACRUZ, MEXICO\\n2023-11-01 06:01:15\\n9 hr 13 min ago\\n17.640 -101.280 49 3.9 GUERRERO, MEXICO\\n2023-11-01 05:54:42\\n9 hr 20 min ago\\n18.720 -101.760 67 3.9 MICHOACAN, MEXICO\\n2023-11-01 05:51:13\\n9 hr 23 min ago\\n-31.330 139.230 16 2.9 SOUTH AUSTRALIA\\n2023-11-01 05:46:17\\n9 hr 28 min ago\\n4.820 96.130 10 3.7 NORTHERN SUMATRA, INDONESIA\\n2023-11-01 05:39:11\\n9 hr 36 min ago\\n8.980 126.500 26 3.1 MINDANAO, PHILIPPINES\\n2023-11-01 05:30:13\\n9 hr 44 min ago\\n46.337 7.470 3 0.9 SWITZERLAND\\n2023-11-01 05:24:13\\n9 hr 50 min ago\\n38.260 20.430 5 2.0 GREECE'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuerpo = tabla.find_element(By.TAG_NAME, 'tbody').text\n",
    "cuerpo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "filas = tabla.find_elements(By.TAG_NAME, 'tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [['2023-11-01 15:14:13', '27 min ago'],\n",
       "  ['36.891'],\n",
       "  ['-2.999'],\n",
       "  ['13'],\n",
       "  ['2.2'],\n",
       "  ['STRAIT OF GIBRALTAR']],\n",
       " [['2023-11-01 15:09:09', '33 min ago'],\n",
       "  ['11.840'],\n",
       "  ['125.640'],\n",
       "  ['19'],\n",
       "  ['3.0'],\n",
       "  ['SAMAR, PHILIPPINES']],\n",
       " [['2023-11-01 15:03:32', '38 min ago'],\n",
       "  ['40.528'],\n",
       "  ['-123.568'],\n",
       "  ['5'],\n",
       "  ['2.4'],\n",
       "  ['NORTHERN CALIFORNIA']],\n",
       " [['2023-11-01 14:49:46', '52 min ago'],\n",
       "  ['59.976'],\n",
       "  ['10.946'],\n",
       "  ['8'],\n",
       "  ['3.0'],\n",
       "  ['SOUTHERN NORWAY']],\n",
       " [['2023-11-01 14:45:25', '56 min ago'],\n",
       "  ['0.140'],\n",
       "  ['124.260'],\n",
       "  ['75'],\n",
       "  ['3.5'],\n",
       "  ['MINAHASA, SULAWESI, INDONESIA']],\n",
       " [['2023-11-01 14:38:02', '1 hr 04 min ago'],\n",
       "  ['36.839'],\n",
       "  ['-3.007'],\n",
       "  ['13'],\n",
       "  ['1.8'],\n",
       "  ['STRAIT OF GIBRALTAR']],\n",
       " [['2023-11-01 14:35:25', '1 hr 06 min ago'],\n",
       "  ['13.820'],\n",
       "  ['120.520'],\n",
       "  ['50'],\n",
       "  ['3.6'],\n",
       "  ['MINDORO, PHILIPPINES']],\n",
       " [['2023-11-01 14:33:41', '1 hr 08 min ago'],\n",
       "  ['37.610'],\n",
       "  ['21.810'],\n",
       "  ['5'],\n",
       "  ['2.5'],\n",
       "  ['SOUTHERN GREECE']],\n",
       " [['2023-11-01 14:30:21', '1 hr 11 min ago'],\n",
       "  ['19.402'],\n",
       "  ['-155.264'],\n",
       "  ['4'],\n",
       "  ['2.0'],\n",
       "  ['ISLAND OF HAWAII, HAWAII']],\n",
       " [['2023-11-01 14:29:27', '1 hr 12 min ago'],\n",
       "  ['37.875'],\n",
       "  ['36.335'],\n",
       "  ['3'],\n",
       "  ['2.1'],\n",
       "  ['CENTRAL TURKEY']],\n",
       " [['2023-11-01 14:26:30', '1 hr 15 min ago'],\n",
       "  ['-24.003'],\n",
       "  ['-66.809'],\n",
       "  ['218'],\n",
       "  ['3.0'],\n",
       "  ['JUJUY, ARGENTINA']],\n",
       " [['2023-11-01 14:10:58', '1 hr 31 min ago'],\n",
       "  ['19.483'],\n",
       "  ['-155.489'],\n",
       "  ['-0'],\n",
       "  ['2.5'],\n",
       "  ['ISLAND OF HAWAII, HAWAII']],\n",
       " [['2023-11-01 14:09:01', '1 hr 33 min ago'],\n",
       "  ['36.796'],\n",
       "  ['-3.026'],\n",
       "  ['9'],\n",
       "  ['2.2'],\n",
       "  ['STRAIT OF GIBRALTAR']],\n",
       " [['2023-11-01 14:07:48', '1 hr 34 min ago'],\n",
       "  ['-8.333'],\n",
       "  ['121.545'],\n",
       "  ['13'],\n",
       "  ['5.1'],\n",
       "  ['FLORES REGION, INDONESIA']],\n",
       " [['2023-11-01 14:00:40', '1 hr 41 min ago'],\n",
       "  ['45.943'],\n",
       "  ['13.823'],\n",
       "  ['6'],\n",
       "  ['1.2'],\n",
       "  ['SLOVENIA']],\n",
       " [['2023-11-01 13:57:28', '1 hr 44 min ago'],\n",
       "  ['37.870'],\n",
       "  ['36.359'],\n",
       "  ['5'],\n",
       "  ['2.7'],\n",
       "  ['CENTRAL TURKEY']],\n",
       " [['2023-11-01 13:50:15', '1 hr 51 min ago'],\n",
       "  ['45.937'],\n",
       "  ['10.947'],\n",
       "  ['6'],\n",
       "  ['1.1'],\n",
       "  ['NORTHERN ITALY']],\n",
       " [['2023-11-01 13:46:29', '1 hr 55 min ago'],\n",
       "  ['39.161'],\n",
       "  ['20.542'],\n",
       "  ['4'],\n",
       "  ['2.0'],\n",
       "  ['GREECE']],\n",
       " [['2023-11-01 13:44:37', '1 hr 57 min ago'],\n",
       "  ['-2.960'],\n",
       "  ['128.840'],\n",
       "  ['10'],\n",
       "  ['2.6'],\n",
       "  ['CERAM SEA, INDONESIA']],\n",
       " [['2023-11-01 13:43:45', '1 hr 58 min ago'],\n",
       "  ['35.760'],\n",
       "  ['26.920'],\n",
       "  ['33'],\n",
       "  ['2.2'],\n",
       "  ['CRETE, GREECE']]]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado = []\n",
    "\n",
    "\n",
    "for f in filas:\n",
    "    \n",
    "    tmp = []\n",
    "    \n",
    "    elementos = f.find_elements(By.TAG_NAME, 'td')  \n",
    "    \n",
    "    for e in elementos:\n",
    "         if e.text != \"\":\n",
    "                tmp.append(e.text.split('\\n'))\n",
    "        \n",
    "        \n",
    "    resultado.append(tmp)\n",
    "    \n",
    "resultado[:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Date & Time',\n",
       "  'Lat. Degrees',\n",
       "  'Lon. Degrees',\n",
       "  'Depth km',\n",
       "  'Mag.+',\n",
       "  'Region']]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnas=[['Date & Time','Lat. Degrees','Lon. Degrees','Depth km', 'Mag.+', 'Region']]\n",
    "columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Date &amp; Time</th>\n",
       "      <th>Lat. Degrees</th>\n",
       "      <th>Lon. Degrees</th>\n",
       "      <th>Depth km</th>\n",
       "      <th>Mag.+</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2023-11-01 15:14:13, 27 min ago]</td>\n",
       "      <td>[36.891]</td>\n",
       "      <td>[-2.999]</td>\n",
       "      <td>[13]</td>\n",
       "      <td>[2.2]</td>\n",
       "      <td>[STRAIT OF GIBRALTAR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2023-11-01 15:09:09, 33 min ago]</td>\n",
       "      <td>[11.840]</td>\n",
       "      <td>[125.640]</td>\n",
       "      <td>[19]</td>\n",
       "      <td>[3.0]</td>\n",
       "      <td>[SAMAR, PHILIPPINES]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2023-11-01 15:03:32, 38 min ago]</td>\n",
       "      <td>[40.528]</td>\n",
       "      <td>[-123.568]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2.4]</td>\n",
       "      <td>[NORTHERN CALIFORNIA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2023-11-01 14:49:46, 52 min ago]</td>\n",
       "      <td>[59.976]</td>\n",
       "      <td>[10.946]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[3.0]</td>\n",
       "      <td>[SOUTHERN NORWAY]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>[2023-11-01 05:51:13, 9 hr 51 min ago]</td>\n",
       "      <td>[-31.330]</td>\n",
       "      <td>[139.230]</td>\n",
       "      <td>[16]</td>\n",
       "      <td>[2.9]</td>\n",
       "      <td>[SOUTH AUSTRALIA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>[2023-11-01 05:46:17, 9 hr 56 min ago]</td>\n",
       "      <td>[4.820]</td>\n",
       "      <td>[96.130]</td>\n",
       "      <td>[10]</td>\n",
       "      <td>[3.7]</td>\n",
       "      <td>[NORTHERN SUMATRA, INDONESIA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>[2023-11-01 05:39:11, 10 hr 04 min ago]</td>\n",
       "      <td>[8.980]</td>\n",
       "      <td>[126.500]</td>\n",
       "      <td>[26]</td>\n",
       "      <td>[3.1]</td>\n",
       "      <td>[MINDANAO, PHILIPPINES]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>[2023-11-01 05:30:13, 10 hr 12 min ago]</td>\n",
       "      <td>[46.337]</td>\n",
       "      <td>[7.470]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[0.9]</td>\n",
       "      <td>[SWITZERLAND]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>[2023-11-01 05:24:13, 10 hr 18 min ago]</td>\n",
       "      <td>[38.260]</td>\n",
       "      <td>[20.430]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[GREECE]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Date & Time Lat. Degrees Lon. Degrees  \\\n",
       "0                                       None         None         None   \n",
       "1          [2023-11-01 15:14:13, 27 min ago]     [36.891]     [-2.999]   \n",
       "2          [2023-11-01 15:09:09, 33 min ago]     [11.840]    [125.640]   \n",
       "3          [2023-11-01 15:03:32, 38 min ago]     [40.528]   [-123.568]   \n",
       "4          [2023-11-01 14:49:46, 52 min ago]     [59.976]     [10.946]   \n",
       "..                                       ...          ...          ...   \n",
       "102   [2023-11-01 05:51:13, 9 hr 51 min ago]    [-31.330]    [139.230]   \n",
       "103   [2023-11-01 05:46:17, 9 hr 56 min ago]      [4.820]     [96.130]   \n",
       "104  [2023-11-01 05:39:11, 10 hr 04 min ago]      [8.980]    [126.500]   \n",
       "105  [2023-11-01 05:30:13, 10 hr 12 min ago]     [46.337]      [7.470]   \n",
       "106  [2023-11-01 05:24:13, 10 hr 18 min ago]     [38.260]     [20.430]   \n",
       "\n",
       "    Depth km  Mag.+                         Region  \n",
       "0       None   None                           None  \n",
       "1       [13]  [2.2]          [STRAIT OF GIBRALTAR]  \n",
       "2       [19]  [3.0]           [SAMAR, PHILIPPINES]  \n",
       "3        [5]  [2.4]          [NORTHERN CALIFORNIA]  \n",
       "4        [8]  [3.0]              [SOUTHERN NORWAY]  \n",
       "..       ...    ...                            ...  \n",
       "102     [16]  [2.9]              [SOUTH AUSTRALIA]  \n",
       "103     [10]  [3.7]  [NORTHERN SUMATRA, INDONESIA]  \n",
       "104     [26]  [3.1]        [MINDANAO, PHILIPPINES]  \n",
       "105      [3]  [0.9]                  [SWITZERLAND]  \n",
       "106      [5]  [2.0]                       [GREECE]  \n",
       "\n",
       "[107 rows x 6 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(resultado, columns=columnas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/elonmusk?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "driver.find_element(By.XPATH, '//*[@id=\"layers\"]/div[2]/div/div/div/div/div/div[2]/div[2]/div/div[2]/div/div[2]/div[2]/div[2]/div/span/span').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'32,7 mil posts'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post=driver.find_element(By.CSS_SELECTOR,'#react-root > div > div > div.css-1dbjc4n.r-18u37iz.r-13qz1uu.r-417010 > main > div > div > div > div > div > div.css-1dbjc4n.r-aqfbo4.r-gtdqiz.r-1gn8etr.r-1g40b8q > div:nth-child(1) > div > div > div > div > div > div.css-1dbjc4n.r-16y2uox.r-1wbh5a2.r-1pi2tsx.r-1777fci > div > div')\n",
    "\n",
    "post.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selenium.webdriver.remote.webelement.WebElement"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/elonmusk?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'161,7 M'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "driver.find_element(By.CSS_SELECTOR,'#react-root > div > div > div.css-1dbjc4n.r-18u37iz.r-13qz1uu.r-417010 > main > div > div > div > div > div > div:nth-child(3) > div > div > div > div > div.css-1dbjc4n.r-13awgt0.r-18u37iz.r-1w6e6rj > div:nth-child(2) > a > span.css-901oao.css-16my406.r-18jsvk2.r-poiln3.r-1b43r93.r-b88u0q.r-1cwl3u0.r-bcqeeo.r-qvutc0 > span').text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'161,7 M Seguidores'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.find_element(By.XPATH,'//*[@id=\"react-root\"]/div/div/div[2]/main/div/div/div/div/div/div[3]/div/div/div/div/div[4]/div[2]/a').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "idiomas = [e.text for e in driver.find_elements(By.XPATH, '//a//strong')]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "numero_articulos = [e.text for e in driver.find_elements(By.XPATH, '//a//small')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Español', '1 892 000+ artículos'),\n",
       " ('English', '6 715 000+ articles'),\n",
       " ('日本語', '1 387 000+ 記事'),\n",
       " ('Русский', '1 938 000+ статей'),\n",
       " ('Deutsch', '2 836 000+ Artikel'),\n",
       " ('Français', '2 553 000+ articles'),\n",
       " ('Italiano', '1 826 000+ voci'),\n",
       " ('中文', '1 377 000+ 条目 / 條目'),\n",
       " ('Português', '1 109 000+ artigos'),\n",
       " ('العربية', '1 217 000+ مقالة')]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado= list(zip(idiomas,numero_articulos))\n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code \n",
    "driver.find_element(By.XPATH, '//*[@id=\"global-cookie-message\"]/div[1]/div/div[2]/button[1]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH, '//*[@id=\"global-cookie-message\"]/div[2]/div/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business and economy\n",
      "Crime and justice\n",
      "Defence\n",
      "Education\n",
      "Environment\n",
      "Government\n",
      "Government spending\n",
      "Health\n",
      "Mapping\n",
      "Society\n",
      "Towns and cities\n",
      "Transport\n",
      "Digital service performance\n",
      "Government reference data\n"
     ]
    }
   ],
   "source": [
    "heading=driver.find_elements(By.XPATH, '//h3//a')\n",
    "\n",
    "for e in heading:\n",
    "    print(e.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
