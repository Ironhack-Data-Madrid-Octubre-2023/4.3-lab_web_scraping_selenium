{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Selenium Doc](https://www.selenium.dev/documentation/)\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `Selenium` and `pandas` are imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable the options you may need. In the next cell you have an example of them but you can choose to use them or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#driver configuration\\nopciones=Options()\\n\\nopciones.add_experimental_option('excludeSwitches', ['enable-automation'])\\nopciones.add_experimental_option('useAutomationExtension', False)\\nopciones.headless=False    # si True, no aperece la ventana (headless=no visible)\\nopciones.add_argument('--start-maximized')         # comienza maximizado\\n#opciones.add_argument('user-data-dir=selenium')    # mantiene las cookies\\n#opciones.add_extension('driver_folder/adblock.crx')       # adblocker\\nopciones.add_argument('--incognito')\\n\""
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#driver configuration\n",
    "opciones=Options()\n",
    "\n",
    "opciones.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "opciones.add_experimental_option('useAutomationExtension', False)\n",
    "opciones.headless=False    # si True, no aperece la ventana (headless=no visible)\n",
    "opciones.add_argument('--start-maximized')         # comienza maximizado\n",
    "#opciones.add_argument('user-data-dir=selenium')    # mantiene las cookies\n",
    "#opciones.add_extension('driver_folder/adblock.crx')       # adblocker\n",
    "opciones.add_argument('--incognito')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matia\\AppData\\Local\\Temp\\ipykernel_9640\\1117115771.py:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(PATH)\n"
     ]
    }
   ],
   "source": [
    "PATH = 'driver/chromedriver'\n",
    "\n",
    "driver = webdriver.Chrome(PATH) \n",
    "\n",
    "driver = webdriver.Chrome(options = opciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse, and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=req.get(url).text\n",
    "\n",
    "sopa=bs(html, 'html.parser')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matia\\AppData\\Local\\Temp\\ipykernel_9640\\624065238.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(PATH, options=opciones)\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(PATH, options=opciones)\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ismail Pelaseyed', 'homanp'],\n",
       " ['Chris Banes', 'chrisbanes'],\n",
       " ['Travis Cline', 'tmc'],\n",
       " ['Xiaoyu Zhang', 'BBuf'],\n",
       " ['Stefan Prodan', 'stefanprodan'],\n",
       " ['Pedro Cattori', 'pcattori'],\n",
       " ['lllyasviel'],\n",
       " ['Arvin Xu', 'arvinxx'],\n",
       " ['Howard Wu', 'howardwu'],\n",
       " ['Shahed Nasser', 'shahednasser'],\n",
       " ['Kailash Nadh', 'knadh'],\n",
       " ['Mattt', 'mattt'],\n",
       " ['kixelated'],\n",
       " ['Brad Fitzpatrick', 'bradfitz'],\n",
       " ['Miško Hevery', 'mhevery'],\n",
       " ['Andrew Lock', 'andrewlock'],\n",
       " ['Brian Smith', 'briansmith'],\n",
       " ['Numan', 'numandev1'],\n",
       " ['Fons van der Plas', 'fonsp'],\n",
       " ['Leonid Bugaev', 'buger'],\n",
       " ['wū yāng', 'uyarn'],\n",
       " ['Steven Nguyen', 'stnguyen90'],\n",
       " ['Argo Zhang', 'ArgoZhang'],\n",
       " ['Josh Stein', 'jcstein'],\n",
       " ['Mike McNeil', 'mikermcneil']]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.text.split('\\n')for e in driver.find_elements(By.CSS_SELECTOR ,' div.d-sm-flex.flex-auto > div.col-sm-8.d-md-flex > div:nth-child(1)')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['swisskyrepo / PayloadsAllTheThings',\n",
       " 'THUDM / ChatGLM3',\n",
       " 'PaddlePaddle / PaddleOCR',\n",
       " 'langchain-ai / langchain',\n",
       " 'dataelement / bisheng',\n",
       " 'Azure / MS-AMP',\n",
       " 'xxlong0 / Wonder3D',\n",
       " 'yunjey / pytorch-tutorial',\n",
       " 'facebookresearch / llama',\n",
       " 'localstack / localstack',\n",
       " 'OpenBMB / ChatDev',\n",
       " 'ray-project / ray',\n",
       " 'CycodeLabs / raven',\n",
       " 'ageitgey / face_recognition',\n",
       " 'iam-veeramalla / aws-devops-zero-to-hero',\n",
       " 'openai / whisper',\n",
       " 'sqlfluff / sqlfluff',\n",
       " 'imartinez / privateGPT',\n",
       " 'reflex-dev / reflex',\n",
       " 'donnemartin / system-design-primer',\n",
       " 'lm-sys / FastChat',\n",
       " 'togethercomputer / RedPajama-Data',\n",
       " 'SkyworkAI / Skywork',\n",
       " 'microsoft / qlib',\n",
       " 'pathwaycom / llm-app']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =driver.find_elements(By.CSS_SELECTOR, 'h2 > a')\n",
    "\n",
    "repo = [e.text for e in x]\n",
    "repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "fotos = driver.find_elements(By.TAG_NAME, 'img')\n",
    "links= [e.get_attribute('src') for e in fotos]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' \n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = driver.find_elements(By.TAG_NAME, 'a')\n",
    "links= [e.get_attribute('href') for e in wiki]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title 1 - General Provisions ٭',\n",
       " 'Title 2 - The Congress',\n",
       " 'Title 3 - The President ٭',\n",
       " 'Title 4 - Flag and Seal, Seat of Government, and the States ٭',\n",
       " 'Title 5 - Government Organization and Employees ٭',\n",
       " 'Title 6 - Domestic Security',\n",
       " 'Title 7 - Agriculture',\n",
       " 'Title 8 - Aliens and Nationality',\n",
       " 'Title 9 - Arbitration ٭',\n",
       " 'Title 10 - Armed Forces ٭',\n",
       " 'Title 11 - Bankruptcy ٭',\n",
       " 'Title 12 - Banks and Banking',\n",
       " 'Title 13 - Census ٭',\n",
       " 'Title 14 - Coast Guard ٭',\n",
       " 'Title 15 - Commerce and Trade',\n",
       " 'Title 16 - Conservation',\n",
       " 'Title 17 - Copyrights ٭',\n",
       " 'Title 18 - Crimes and Criminal Procedure ٭',\n",
       " 'Title 19 - Customs Duties',\n",
       " 'Title 21 - Food and Drugs',\n",
       " 'Title 22 - Foreign Relations and Intercourse',\n",
       " 'Title 23 - Highways ٭',\n",
       " 'Title 24 - Hospitals and Asylums',\n",
       " 'Title 25 - Indians',\n",
       " 'Title 26 - Internal Revenue Code',\n",
       " 'Title 27 - Intoxicating Liquors',\n",
       " 'Title 28 - Judiciary and Judicial Procedure ٭',\n",
       " 'Title 29 - Labor',\n",
       " 'Title 30 - Mineral Lands and Mining',\n",
       " 'Title 31 - Money and Finance ٭',\n",
       " 'Title 32 - National Guard ٭',\n",
       " 'Title 33 - Navigation and Navigable Waters',\n",
       " 'Title 34 - Crime Control and Law Enforcement',\n",
       " 'Title 35 - Patents ٭',\n",
       " 'Title 36 - Patriotic and National Observances, Ceremonies, and Organizations ٭',\n",
       " 'Title 37 - Pay and Allowances of the Uniformed Services ٭',\n",
       " 'Title 39 - Postal Service ٭',\n",
       " 'Title 40 - Public Buildings, Property, and Works ٭',\n",
       " 'Title 41 - Public Contracts ٭',\n",
       " 'Title 42 - The Public Health and Welfare',\n",
       " 'Title 43 - Public Lands',\n",
       " 'Title 44 - Public Printing and Documents ٭',\n",
       " 'Title 45 - Railroads',\n",
       " 'Title 46 - Shipping ٭',\n",
       " 'Title 47 - Telecommunications',\n",
       " 'Title 48 - Territories and Insular Possessions',\n",
       " 'Title 49 - Transportation ٭',\n",
       " 'Title 50 - War and National Defense',\n",
       " 'Title 51 - National and Commercial Space Programs ٭',\n",
       " 'Title 52 - Voting and Elections',\n",
       " 'Title 53 [Reserved]',\n",
       " 'Title 54 - National Park Service and Related Programs ٭']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =driver.find_elements(By.CLASS_NAME, 'usctitle')\n",
    "titulos = [e.text for e in x][2:]\n",
    "titulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARNOLDO JIMENEZ',\n",
       " 'OMAR ALEXANDER CARDENAS',\n",
       " 'YULAN ADONAY ARCHAGA CARIAS',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'DONALD EUGENE FIELDS II',\n",
       " 'RUJA IGNATOVA',\n",
       " 'WILVER VILLEGAS-PALOMINO',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ']"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = driver.find_elements(By.CSS_SELECTOR, ' h3 > a')\n",
    "top_chorros = [e.text for e in x]\n",
    "top_chorros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2023-11-01 15:14:13',\n",
       "  '11 min ago',\n",
       "  '36.891 -2.999 13 2.2 STRAIT OF GIBRALTAR'],\n",
       " ['2023-11-01 15:09:09',\n",
       "  '16 min ago',\n",
       "  '11.840 125.640 19 3.0 SAMAR, PHILIPPINES'],\n",
       " ['2023-11-01 15:03:32',\n",
       "  '22 min ago',\n",
       "  '40.528 -123.568 5 2.4 NORTHERN CALIFORNIA'],\n",
       " ['2023-11-01 14:49:46', '36 min ago', '59.976 10.946 8 3.0 SOUTHERN NORWAY'],\n",
       " ['2023-11-01 14:45:25',\n",
       "  '40 min ago',\n",
       "  '0.140 124.260 75 3.5 MINAHASA, SULAWESI, INDONESIA'],\n",
       " ['2023-11-01 14:38:02',\n",
       "  '47 min ago',\n",
       "  '36.839 -3.007 13 1.8 STRAIT OF GIBRALTAR'],\n",
       " ['2023-11-01 14:35:25',\n",
       "  '50 min ago',\n",
       "  '13.820 120.520 50 3.6 MINDORO, PHILIPPINES'],\n",
       " ['2023-11-01 14:33:41', '52 min ago', '37.610 21.810 5 2.5 SOUTHERN GREECE'],\n",
       " ['2023-11-01 14:30:21',\n",
       "  '55 min ago',\n",
       "  '19.402 -155.264 4 2.0 ISLAND OF HAWAII, HAWAII'],\n",
       " ['2023-11-01 14:29:27', '56 min ago', '37.875 36.335 3 2.1 CENTRAL TURKEY'],\n",
       " ['2023-11-01 14:26:30',\n",
       "  '59 min ago',\n",
       "  '-24.003 -66.809 218 3.0 JUJUY, ARGENTINA'],\n",
       " ['2023-11-01 14:10:58',\n",
       "  '1 hr 15 min ago',\n",
       "  '19.483 -155.489 -0 2.5 ISLAND OF HAWAII, HAWAII'],\n",
       " ['2023-11-01 14:09:01',\n",
       "  '1 hr 16 min ago',\n",
       "  '36.796 -3.026 9 2.2 STRAIT OF GIBRALTAR'],\n",
       " ['2023-11-01 14:07:48',\n",
       "  '1 hr 18 min ago',\n",
       "  '-8.333 121.545 13 5.1 FLORES REGION, INDONESIA'],\n",
       " ['2023-11-01 14:00:40', '1 hr 25 min ago', '45.943 13.823 6 1.2 SLOVENIA'],\n",
       " ['2023-11-01 13:57:28',\n",
       "  '1 hr 28 min ago',\n",
       "  '37.870 36.359 5 2.7 CENTRAL TURKEY'],\n",
       " ['2023-11-01 13:50:15',\n",
       "  '1 hr 35 min ago',\n",
       "  '45.937 10.947 6 1.1 NORTHERN ITALY'],\n",
       " ['2023-11-01 13:46:29', '1 hr 39 min ago', '39.161 20.542 4 2.0 GREECE'],\n",
       " ['2023-11-01 13:44:37',\n",
       "  '1 hr 41 min ago',\n",
       "  '-2.960 128.840 10 2.6 CERAM SEA, INDONESIA'],\n",
       " ['2023-11-01 13:43:45',\n",
       "  '1 hr 42 min ago',\n",
       "  '35.760 26.920 33 2.2 CRETE, GREECE'],\n",
       " ['2023-11-01 13:27:52',\n",
       "  '1 hr 58 min ago',\n",
       "  '41.439 28.428 3 2.4 WESTERN TURKEY'],\n",
       " ['2023-11-01 13:25:48',\n",
       "  '2 hr 00 min ago',\n",
       "  '-2.820 135.470 10 3.8 NEAR N COAST OF PAPUA, INDONESIA'],\n",
       " ['2023-11-01 13:22:17',\n",
       "  '2 hr 03 min ago',\n",
       "  '-20.340 -69.000 102 2.9 TARAPACA, CHILE'],\n",
       " ['2023-11-01 13:11:43',\n",
       "  '2 hr 14 min ago',\n",
       "  '48.640 155.950 10 4.0 KURIL ISLANDS'],\n",
       " ['2023-11-01 12:58:48',\n",
       "  '2 hr 27 min ago',\n",
       "  '36.036 36.103 5 2.9 TURKEY-SYRIA BORDER REGION'],\n",
       " ['2023-11-01 12:53:08', '2 hr 32 min ago', '38.080 23.480 7 2.0 GREECE'],\n",
       " ['2023-11-01 12:51:15',\n",
       "  '2 hr 34 min ago',\n",
       "  '10.925 93.060 94 5.1 ANDAMAN ISLANDS, INDIA REGION'],\n",
       " ['2023-11-01 12:49:31',\n",
       "  '2 hr 36 min ago',\n",
       "  '-12.400 -77.020 54 3.7 NEAR COAST OF CENTRAL PERU'],\n",
       " ['2023-11-01 12:48:50',\n",
       "  '2 hr 37 min ago',\n",
       "  '-5.090 104.130 10 2.5 SOUTHERN SUMATRA, INDONESIA'],\n",
       " ['2023-11-01 12:31:58', '2 hr 54 min ago', '38.890 23.310 10 2.0 GREECE'],\n",
       " ['2023-11-01 12:25:13', '3 hr 00 min ago', '50.306 12.367 10 2.4 GERMANY'],\n",
       " ['2023-11-01 12:22:38',\n",
       "  '3 hr 03 min ago',\n",
       "  '36.765 -3.002 11 2.2 STRAIT OF GIBRALTAR'],\n",
       " ['2023-11-01 12:01:45',\n",
       "  '3 hr 24 min ago',\n",
       "  '37.470 22.230 194 2.6 SOUTHERN GREECE'],\n",
       " ['2023-11-01 12:00:41',\n",
       "  '3 hr 25 min ago',\n",
       "  '11.850 -86.440 79 2.7 NEAR COAST OF NICARAGUA'],\n",
       " ['2023-11-01 11:54:44',\n",
       "  '3 hr 31 min ago',\n",
       "  '37.466 37.274 2 2.4 CENTRAL TURKEY'],\n",
       " ['2023-11-01 11:52:50',\n",
       "  '3 hr 33 min ago',\n",
       "  '38.775 40.173 13 2.4 EASTERN TURKEY'],\n",
       " ['2023-11-01 11:40:41',\n",
       "  '3 hr 45 min ago',\n",
       "  '-8.970 109.890 10 3.3 JAVA, INDONESIA'],\n",
       " ['2023-11-01 11:35:37',\n",
       "  '3 hr 50 min ago',\n",
       "  '61.345 -146.956 32 2.5 SOUTHERN ALASKA'],\n",
       " ['2023-11-01 11:35:22',\n",
       "  '3 hr 50 min ago',\n",
       "  '-8.920 109.990 10 3.2 JAVA, INDONESIA'],\n",
       " ['2023-11-01 11:27:11',\n",
       "  '3 hr 58 min ago',\n",
       "  '-8.030 107.370 23 3.2 JAVA, INDONESIA'],\n",
       " ['2023-11-01 11:24:04', '4 hr 01 min ago', '37.059 -1.885 0 1.5 SPAIN'],\n",
       " ['2023-11-01 11:15:01', '4 hr 10 min ago', '36.659 -116.267 8 2.7 NEVADA'],\n",
       " ['2023-11-01 11:10:53',\n",
       "  '4 hr 15 min ago',\n",
       "  '38.816 -122.818 2 2.0 NORTHERN CALIFORNIA'],\n",
       " ['2023-11-01 10:54:24',\n",
       "  '4 hr 31 min ago',\n",
       "  '18.650 -103.260 11 3.7 MICHOACAN, MEXICO'],\n",
       " ['2023-11-01 10:47:26', '4 hr 38 min ago', '47.353 6.911 4 1.9 FRANCE'],\n",
       " ['2023-11-01 10:46:13', '4 hr 39 min ago', '35.858 -98.049 5 2.3 OKLAHOMA'],\n",
       " ['2023-11-01 10:40:15',\n",
       "  '4 hr 45 min ago',\n",
       "  '1.549 129.288 10 4.9 HALMAHERA, INDONESIA'],\n",
       " ['2023-11-01 10:39:47',\n",
       "  '4 hr 46 min ago',\n",
       "  '32.110 59.730 10 4.7 EASTERN IRAN'],\n",
       " ['2023-11-01 10:29:47',\n",
       "  '4 hr 56 min ago',\n",
       "  '19.650 -104.280 45 3.6 JALISCO, MEXICO'],\n",
       " ['2023-11-01 10:24:52',\n",
       "  '5 hr 01 min ago',\n",
       "  '-35.756 179.285 239 3.3 OFF E. COAST OF N. ISLAND, N.Z.'],\n",
       " ['2023-11-01 10:18:14', '5 hr 07 min ago', '46.305 13.600 6 1.3 SLOVENIA'],\n",
       " ['2023-11-01 10:15:43',\n",
       "  '5 hr 10 min ago',\n",
       "  '-15.517 -177.518 410 5.1 FIJI REGION'],\n",
       " ['2023-11-01 10:12:56',\n",
       "  '5 hr 13 min ago',\n",
       "  '17.810 -103.020 16 3.8 OFFSHORE MICHOACAN, MEXICO'],\n",
       " ['2023-11-01 10:06:48',\n",
       "  '5 hr 19 min ago',\n",
       "  '35.364 33.441 24 1.5 CYPRUS REGION'],\n",
       " ['2023-11-01 09:57:14',\n",
       "  '5 hr 28 min ago',\n",
       "  '17.932 -66.287 13 2.7 PUERTO RICO REGION'],\n",
       " ['2023-11-01 09:40:30',\n",
       "  '5 hr 45 min ago',\n",
       "  '-31.770 -69.090 90 3.1 SAN JUAN, ARGENTINA'],\n",
       " ['2023-11-01 09:40:23',\n",
       "  '5 hr 45 min ago',\n",
       "  '-2.110 120.920 10 3.3 SULAWESI, INDONESIA'],\n",
       " ['2023-11-01 09:36:41',\n",
       "  '5 hr 49 min ago',\n",
       "  '16.950 -99.640 44 3.7 GUERRERO, MEXICO'],\n",
       " ['2023-11-01 09:35:57',\n",
       "  '5 hr 50 min ago',\n",
       "  '34.830 24.820 20 2.8 CRETE, GREECE'],\n",
       " ['2023-11-01 09:35:48',\n",
       "  '5 hr 50 min ago',\n",
       "  '30.500 -116.180 19 3.6 OFFSHORE BAJA CALIFORNIA, MEXICO'],\n",
       " ['2023-11-01 09:20:50',\n",
       "  '6 hr 05 min ago',\n",
       "  '39.406 28.446 5 2.1 WESTERN TURKEY'],\n",
       " ['2023-11-01 09:15:41',\n",
       "  '6 hr 10 min ago',\n",
       "  '25.620 -100.720 5 3.9 COAHUILA, MEXICO'],\n",
       " ['2023-11-01 09:10:44',\n",
       "  '6 hr 15 min ago',\n",
       "  '27.667 -18.256 41 2.1 CANARY ISLANDS, SPAIN REGION'],\n",
       " ['2023-11-01 08:55:19',\n",
       "  '6 hr 30 min ago',\n",
       "  '37.816 36.459 7 2.7 CENTRAL TURKEY'],\n",
       " ['2023-11-01 08:44:01',\n",
       "  '6 hr 41 min ago',\n",
       "  '39.086 33.067 5 2.2 CENTRAL TURKEY'],\n",
       " ['2023-11-01 08:42:42',\n",
       "  '6 hr 43 min ago',\n",
       "  '-41.665 174.231 12 3.1 COOK STRAIT, NEW ZEALAND'],\n",
       " ['2023-11-01 08:41:30',\n",
       "  '6 hr 44 min ago',\n",
       "  '38.700 142.000 60 3.7 NEAR EAST COAST OF HONSHU, JAPAN'],\n",
       " ['2023-11-01 08:38:36',\n",
       "  '6 hr 47 min ago',\n",
       "  '-31.632 178.218 33 4.4 KERMADEC ISLANDS REGION'],\n",
       " ['2023-11-01 08:22:47', '7 hr 03 min ago', '38.900 22.610 77 2.0 GREECE'],\n",
       " ['2023-11-01 08:18:25',\n",
       "  '7 hr 07 min ago',\n",
       "  '-28.770 -71.500 44 3.5 OFFSHORE ATACAMA, CHILE'],\n",
       " ['2023-11-01 08:16:57',\n",
       "  '7 hr 09 min ago',\n",
       "  '19.241 -155.398 31 2.1 ISLAND OF HAWAII, HAWAII'],\n",
       " ['2023-11-01 08:10:09',\n",
       "  '7 hr 15 min ago',\n",
       "  '14.640 -93.820 24 3.9 OFF COAST OF CHIAPAS, MEXICO'],\n",
       " ['2023-11-01 08:08:47',\n",
       "  '7 hr 17 min ago',\n",
       "  '44.361 148.997 44 4.9 KURIL ISLANDS'],\n",
       " ['2023-11-01 08:06:29',\n",
       "  '7 hr 19 min ago',\n",
       "  '15.980 -94.080 86 3.7 OFFSHORE CHIAPAS, MEXICO'],\n",
       " ['2023-11-01 08:05:18',\n",
       "  '7 hr 20 min ago',\n",
       "  '42.855 13.224 5 2.1 CENTRAL ITALY'],\n",
       " ['2023-11-01 07:57:07',\n",
       "  '7 hr 28 min ago',\n",
       "  '36.104 28.808 8 2.6 DODECANESE IS.-TURKEY BORDER REG'],\n",
       " ['2023-11-01 07:57:07', '7 hr 28 min ago', '39.680 24.350 10 3.0 AEGEAN SEA'],\n",
       " ['2023-11-01 07:56:39',\n",
       "  '7 hr 29 min ago',\n",
       "  '18.390 -102.700 58 3.7 MICHOACAN, MEXICO'],\n",
       " ['2023-11-01 07:53:43',\n",
       "  '7 hr 32 min ago',\n",
       "  '36.113 35.785 7 3.1 CENTRAL TURKEY'],\n",
       " ['2023-11-01 07:44:34', '7 hr 41 min ago', '38.200 22.690 5 2.1 GREECE'],\n",
       " ['2023-11-01 07:43:45',\n",
       "  '7 hr 42 min ago',\n",
       "  '61.543 -147.799 10 3.1 SOUTHERN ALASKA'],\n",
       " ['2023-11-01 07:38:27',\n",
       "  '7 hr 47 min ago',\n",
       "  '53.760 159.460 140 3.5 NEAR EAST COAST OF KAMCHATKA'],\n",
       " ['2023-11-01 07:31:35',\n",
       "  '7 hr 54 min ago',\n",
       "  '0.820 122.550 20 2.5 MINAHASA, SULAWESI, INDONESIA'],\n",
       " ['2023-11-01 07:29:26',\n",
       "  '7 hr 56 min ago',\n",
       "  '17.050 -94.180 140 3.5 OAXACA, MEXICO'],\n",
       " ['2023-11-01 07:19:30',\n",
       "  '8 hr 06 min ago',\n",
       "  '10.931 -86.443 21 3.6 OFF COAST OF COSTA RICA'],\n",
       " ['2023-11-01 07:11:43',\n",
       "  '8 hr 14 min ago',\n",
       "  '-32.850 -178.910 44 4.8 SOUTH OF KERMADEC ISLANDS'],\n",
       " ['2023-11-01 07:10:03',\n",
       "  '8 hr 15 min ago',\n",
       "  '35.619 1.363 0 3.0 NORTHERN ALGERIA'],\n",
       " ['2023-11-01 07:04:41',\n",
       "  '8 hr 21 min ago',\n",
       "  '38.892 15.674 180 2.4 SICILY, ITALY'],\n",
       " ['2023-11-01 07:02:42', '8 hr 23 min ago', '38.840 23.510 5 2.1 GREECE'],\n",
       " ['2023-11-01 06:53:03',\n",
       "  '8 hr 32 min ago',\n",
       "  '38.047 36.861 5 2.2 CENTRAL TURKEY'],\n",
       " ['2023-11-01 06:52:57',\n",
       "  '8 hr 33 min ago',\n",
       "  '33.040 76.070 5 3.2 KASHMIR-INDIA BORDER REGION'],\n",
       " ['2023-11-01 06:51:44',\n",
       "  '8 hr 34 min ago',\n",
       "  '-10.917 166.569 70 5.2 SANTA CRUZ ISLANDS'],\n",
       " ['2023-11-01 06:43:30',\n",
       "  '8 hr 42 min ago',\n",
       "  '8.995 -84.027 26 3.0 OFF COAST OF COSTA RICA'],\n",
       " ['2023-11-01 06:30:33',\n",
       "  '8 hr 55 min ago',\n",
       "  '12.620 -88.180 28 2.7 OFFSHORE EL SALVADOR'],\n",
       " ['2023-11-01 06:29:38',\n",
       "  '8 hr 56 min ago',\n",
       "  '-10.950 166.629 74 4.6 SANTA CRUZ ISLANDS'],\n",
       " ['2023-11-01 06:23:24',\n",
       "  '9 hr 02 min ago',\n",
       "  '18.110 -103.310 14 3.6 OFFSHORE MICHOACAN, MEXICO'],\n",
       " ['2023-11-01 06:22:42',\n",
       "  '9 hr 03 min ago',\n",
       "  '15.280 -93.760 11 3.8 OFFSHORE CHIAPAS, MEXICO'],\n",
       " ['2023-11-01 06:03:35',\n",
       "  '9 hr 22 min ago',\n",
       "  '46.389 12.731 8 1.0 NORTHERN ITALY'],\n",
       " ['2023-11-01 06:03:26',\n",
       "  '9 hr 22 min ago',\n",
       "  '17.400 -94.710 136 3.8 VERACRUZ, MEXICO'],\n",
       " ['2023-11-01 06:01:15',\n",
       "  '9 hr 24 min ago',\n",
       "  '17.640 -101.280 49 3.9 GUERRERO, MEXICO']]"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "x = driver.find_elements(By.CSS_SELECTOR, 'body > div.content > div.htab > table')[0].text.split('\\n')[11:]\n",
    "\n",
    "terremoto=[x[i:i+3] for i in range(0,len(x),3)]\n",
    "\n",
    "terremoto\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No se hace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#No se hace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English',\n",
       " '6 715 000+ articles',\n",
       " '日本語',\n",
       " '1 387 000+ 記事',\n",
       " 'Español',\n",
       " '1 892 000+ artículos',\n",
       " 'Русский',\n",
       " '1 938 000+ статей',\n",
       " 'Deutsch',\n",
       " '2 836 000+ Artikel',\n",
       " 'Français',\n",
       " '2 553 000+ articles',\n",
       " 'Italiano',\n",
       " '1 826 000+ voci',\n",
       " '中文',\n",
       " '1 377 000+ 条目 / 條目',\n",
       " 'Português',\n",
       " '1 109 000+ artigos',\n",
       " 'العربية',\n",
       " '1 217 000+ مقالة']"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = driver.find_elements(By.CSS_SELECTOR, ' div.central-featured')[0].text.split('\\n')\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = driver.find_elements(By.CSS_SELECTOR,  'h3 > a')\n",
    "\n",
    "datasets = [e.text for e in x]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mandarin Chinese\\n(incl. Standard Chinese, but excl. other varieties)': '939',\n",
       " 'Spanish': '485',\n",
       " 'English': '380',\n",
       " 'Hindi\\n(excl. Urdu, and other languages)': '345',\n",
       " 'Portuguese': '236',\n",
       " 'Bengali': '234',\n",
       " 'Russian': '147',\n",
       " 'Japanese': '123',\n",
       " 'Yue Chinese\\n(incl. Cantonese)': '86.1',\n",
       " 'Vietnamese': '85.0'}"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = driver.find_elements(By.CSS_SELECTOR, '  td:nth-child(2)')\n",
    "idiomas = [e.text for e in x]\n",
    "idiomas = idiomas[:10]\n",
    "\n",
    "y = driver.find_elements(By.CSS_SELECTOR, 'td:nth-child(1)')\n",
    "language = [e.text for e in y]\n",
    "language = language[:10]\n",
    "\n",
    "top= dict(zip(language,idiomas))\n",
    "top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
